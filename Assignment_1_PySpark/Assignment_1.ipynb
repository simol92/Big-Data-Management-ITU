{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181bbe29-26f4-4c4a-a9e9-0053263bff52",
   "metadata": {},
   "source": [
    "# Assignment: Scalable Processing\n",
    "## Yelp Reviews and Authenticity\n",
    "\n",
    "Big Data Management | by ___ | ____@itu.dk | date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92690678-bb97-43d7-ab27-4a6f8db87079",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connecting to the Spark Cluster job using the two JobParameters.json\n",
    "\n",
    "To connect this jupyter notebook with your Spark cluster, we need to tell jupyter how it can access the spark cluster. Below code accomplishes that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d483a774-a0ca-4873-a2ee-51d2fcd30ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:04:30.865111Z",
     "iopub.status.busy": "2023-12-22T21:04:30.864878Z",
     "iopub.status.idle": "2023-12-22T21:04:45.190064Z",
     "shell.execute_reply": "2023-12-22T21:04:45.188817Z",
     "shell.execute_reply.started": "2023-12-22T21:04:30.865087Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# DO NOT CHANGE ANYTHING HERE.\n",
    "# IF YOU HAVE PROBLEMS, CHECK THE ASSIGNMENT GUIDE CAREFULLY \n",
    "#####################################################################\n",
    "    \n",
    "FIRST_EXECUTION = False\n",
    "# Only execute this cell once.\n",
    "if not FIRST_EXECUTION:\n",
    "    FIRST_EXECUTION = True\n",
    "    import os, json, pyspark\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession, functions as F\n",
    "    from pyspark.sql.functions import avg\n",
    "\n",
    "    # Two files are automatically read: JobParameters.json for the Spark Cluster job using a temporary spark instance\n",
    "    # and JobParameters.json for the Jupyter Lab job to extract the hostname of the cluster. \n",
    "\n",
    "    MASTER_HOST_NAME = None\n",
    "\n",
    "    # Open the parameters Jupyter Lab app was launched with\n",
    "    with open('/work/JobParameters.json', 'r') as file:\n",
    "        JUPYTER_LAB_JOB_PARAMS = json.load(file)\n",
    "        # from pprint import pprint; pprint(JUPYTER_LAB_JOB_PARAMS) \n",
    "        for resource in JUPYTER_LAB_JOB_PARAMS['request']['resources']:\n",
    "            if 'hostname' in resource.keys():\n",
    "                MASTER_HOST_NAME = resource['hostname']\n",
    "\n",
    "    MASTER_HOST = f\"spark://{MASTER_HOST_NAME}:7077\"\n",
    "\n",
    "    conf = SparkConf().setAll([\n",
    "            (\"spark.app.name\", 'reading_job_params_app'), \n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "        ])\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "\n",
    "    CLUSTER_PARAMETERS_JSON_DF = spark.read.option(\"multiline\",\"true\").json('/work/JobParameters.json')\n",
    "\n",
    "    # Extract cluster info from the specific JobParameters.json\n",
    "    NODES = CLUSTER_PARAMETERS_JSON_DF.select(\"request.replicas\").first()[0]\n",
    "    CPUS_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.cpu\").first()[0] - 1\n",
    "    MEM_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.memoryInGigs\").first()[0]\n",
    "\n",
    "    CLUSTER_CORES_MAX = CPUS_PER_NODE * NODES\n",
    "    CLUSTER_MEMORY_MAX = MEM_PER_NODE * NODES \n",
    "\n",
    "    EXECUTOR_CORES = CPUS_PER_NODE - 1  # set cores per executor on worker node\n",
    "    EXECUTOR_MEMORY = int(\n",
    "        MEM_PER_NODE / (CPUS_PER_NODE / EXECUTOR_CORES) * 0.5\n",
    "    )  # set executor memory in GB on each worker node\n",
    "\n",
    "    # Make sure there is a dir for spark logs\n",
    "    if not os.path.exists('spark_logs'):\n",
    "        os.mkdir('spark_logs')\n",
    "\n",
    "    conf = SparkConf().setAll(\n",
    "        [\n",
    "            (\"spark.app.name\", 'spark_assignment'), # Change to your liking \n",
    "            (\"spark.sql.caseSensitive\", False), # Optional: Make queries strings sensitive to captialization\n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "            (\"spark.cores.max\", CLUSTER_CORES_MAX),\n",
    "            (\"spark.executor.cores\", EXECUTOR_CORES),\n",
    "            (\"spark.executor.memory\", str(EXECUTOR_MEMORY) + \"g\"),\n",
    "            (\"spark.eventLog.enabled\", True),\n",
    "            (\"spark.eventLog.dir\", \"spark_logs\"),\n",
    "            (\"spark.history.fs.logDirectory\", \"spark_logs\"),\n",
    "            (\"spark.deploy.mode\", \"cluster\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## check executor memory, taking into accout 10% of memory overhead (minimum 384 MiB)\n",
    "    CHECK = (CLUSTER_CORES_MAX / EXECUTOR_CORES) * (\n",
    "        EXECUTOR_MEMORY + max(EXECUTOR_MEMORY * 0.10, 0.403)\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        int(CHECK) <= CLUSTER_MEMORY_MAX\n",
    "    ), \"Executor memory larger than cluster total memory!\"\n",
    "\n",
    "    # Stop previous session that was just for loading cluster params\n",
    "    spark.stop()\n",
    "\n",
    "    # Start new session with above config, that has better resource handling\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbb4de-8b40-4364-b8d3-2cc488b16cbf",
   "metadata": {},
   "source": [
    "Click on the \"SparkMonitor\" tab at the top in Jupyter Lab to see the status of running code on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3dec-d95c-44b9-a996-12e97cc34c2b",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Here we specify where the yelp datasets are located on UCloud and read then using the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa96702-5902-4482-9cd2-77d6f5da0f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:04:45.192450Z",
     "iopub.status.busy": "2023-12-22T21:04:45.191714Z",
     "iopub.status.idle": "2023-12-22T21:04:57.814578Z",
     "shell.execute_reply": "2023-12-22T21:04:57.813322Z",
     "shell.execute_reply.started": "2023-12-22T21:04:45.192390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the business and review files\n",
    "# This is the path to the shared datasets provided by adding an the dataset input folder\n",
    "# when submitting the spark cluster job.\n",
    "business = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') # Use the file:/// prefix to indicate we want to read from the cluster's filesystem\n",
    "business = business.persist()\n",
    "\n",
    "# Persist 2 commonly used dataframes since they're used for later computations\n",
    "# https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/\n",
    "\n",
    "users = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "reviews = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "reviews = reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89f6184-410e-4a04-a4a1-7b080353dc1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:04:57.817004Z",
     "iopub.status.busy": "2023-12-22T21:04:57.816243Z",
     "iopub.status.idle": "2023-12-22T21:04:59.211321Z",
     "shell.execute_reply": "2023-12-22T21:04:59.209619Z",
     "shell.execute_reply.started": "2023-12-22T21:04:57.816937Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|XQfwVwDr-v0ZS3_Cb...|   0|2018-07-07 22:09:11|    0|KU_O5udG6zpxOg-Vc...|  3.0|If you decide to ...|     0|mh_-eMZ6K5RLWhZyI...|\n",
      "|7ATYjTIgM3jUlt4UM...|   1|2012-01-03 15:28:18|    0|BiTunyQ73aT9WBnpR...|  5.0|I've taken a lot ...|     1|OyoGAe7OKpv6SyGZT...|\n",
      "|YjUWPpI6HXG530lwP...|   0|2014-02-05 20:30:30|    0|saUsX_uimxRlCVr67...|  3.0|Family diner. Had...|     0|8g_iMtfSiwikVnbP2...|\n",
      "|kxX2SOes4o-D3ZQBk...|   1|2015-01-04 00:01:03|    0|AqPFMleE6RsU23_au...|  5.0|Wow!  Yummy, diff...|     1|_7bHUi9Uuf5__HHc_...|\n",
      "|e4Vwtrqf-wpJfwesg...|   1|2017-01-14 20:54:15|    0|Sx8TMOWLNuJBWer-0...|  4.0|Cute interior and...|     1|bcjbaE6dDog4jkNY9...|\n",
      "|04UD14gamNjLY0IDY...|   1|2015-09-23 23:10:31|    2|JrIxlS1TzJ-iCu79u...|  1.0|I am a long term ...|     1|eUta8W_HdHMXPzLBB...|\n",
      "|gmjsEdUsKpj9Xxu6p...|   0|2015-01-03 23:21:18|    2|6AxgBCNX_PNTOxmbR...|  5.0|Loved this tour! ...|     0|r3zeYsv1XFBRA4dJp...|\n",
      "|LHSTtnW3YHCeUkRDG...|   0|2015-08-07 02:29:16|    0|_ZeMknuYdlQcUqng_...|  5.0|Amazingly amazing...|     2|yfFzsLmaWF2d4Sr0U...|\n",
      "|B5XSoSG3SfvQGtKEG...|   0|2016-03-30 22:46:33|    1|ZKvDG2sBvHVdF5oBN...|  3.0|This easter inste...|     1|wSTuiTk-sKNdcFypr...|\n",
      "|gebiRewfieSdtt17P...|   0|2016-07-25 07:31:06|    0|pUycOfUwM8vqX7KjR...|  3.0|Had a party of 6 ...|     0|59MxRhNVhU9MYndMk...|\n",
      "|uMvVYRgGNXf5boolA...|   0|2015-06-21 14:48:06|    0|rGQRf8UafX7OTlMNN...|  5.0|My experience wit...|     2|1WHRWwQmZOZDAhp2Q...|\n",
      "|EQ-TZ2eeD_E0BHuvo...|   0|2015-08-19 14:31:45|    0|l3Wk_mvAog6XANIuG...|  4.0|Locals recommende...|     0|ZbqSHbgCjzVAqaa7N...|\n",
      "|lj-E32x9_FA7GmUrB...|   0|2014-06-27 22:44:01|    0|XW_LfMv0fV21l9c6x...|  4.0|Love going here f...|     0|9OAtfnWag-ajVxRbU...|\n",
      "|RZtGWDLCAtuipwaZ-...|   0|2009-10-14 19:57:14|    0|8JFGBuHMoiNDyfcxu...|  4.0|Good food--loved ...|     0|smOvOajNG0lS4Pq7d...|\n",
      "|otQS34_MymijPTdNB...|   0|2011-10-27 17:12:05|    2|UBp0zWyH60Hmw6Fsa...|  4.0|The bun makes the...|     0|4Uh27DgGzsp6PqrH9...|\n",
      "|BVndHaLihEYbr76Z0...|   0|2014-10-11 16:22:06|    0|OAhBYw8IQ6wlfw1ow...|  5.0|Great place for b...|     0|1C2lxzUo1Hyye4RFI...|\n",
      "|YtSqYv1Q_pOltsVPS...|   0|2013-06-24 11:21:25|    0|oyaMhzBSwfGgemSGu...|  5.0|Tremendous servic...|     0|Dd1jQj7S-BFGqRbAp...|\n",
      "|rBdG_23USc7DletfZ...|   0|2014-08-10 19:41:43|    0|LnGZB0fjfgeVDVz5I...|  4.0|The hubby and I h...|     1|j2wlzrntrbKwyOcOi...|\n",
      "|CLEWowfkj-wKYJlQD...|   1|2016-03-07 00:02:18|    0|u2vzZaOqJ2feRshaa...|  5.0|I go to blow bar ...|     2|NDZvyYHTUWWu-kqgQ...|\n",
      "|eFvzHawVJofxSnD7T...|   0|2014-11-12 15:30:27|    0|Xs8Z8lmKkosqW5mw_...|  5.0|My absolute favor...|     0|IQsF3Rc6IgCzjVV9D...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get number of rows with no sampling:\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc31d1a2-3571-49cd-af5a-98d6d23027de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:04:59.213998Z",
     "iopub.status.busy": "2023-12-22T21:04:59.213368Z",
     "iopub.status.idle": "2023-12-22T21:04:59.698293Z",
     "shell.execute_reply": "2023-12-22T21:04:59.696764Z",
     "shell.execute_reply.started": "2023-12-22T21:04:59.213946Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+\n",
      "|             address|          attributes|         business_id|          categories|          city|               hours|is_open|     latitude|     longitude|                name|postal_code|review_count|stars|state|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+\n",
      "|1616 Chapala St, ...|{null, null, null...|Pns2l4eNsfO8kk83d...|Doctors, Traditio...| Santa Barbara|                null|      0|   34.4266787|  -119.7111968|Abby Rappoport, L...|      93101|           7|  5.0|   CA|\n",
      "|87 Grasso Plaza S...|{null, null, null...|mpf3x-BjTdTEA3yCZ...|Shipping Centers,...|        Affton|{8:0-18:30, 0:0-0...|      1|    38.551126|    -90.335695|       The UPS Store|      63123|          15|  3.0|   MO|\n",
      "|5255 E Broadway Blvd|{null, null, null...|tUFrWirKiKi_TAnsV...|Department Stores...|        Tucson|{8:0-23:0, 8:0-22...|      0|    32.223236|   -110.880452|              Target|      85711|          22|  3.5|   AZ|\n",
      "|         935 Race St|{null, null, u'no...|MTSW4McQd7CbVtyjq...|Restaurants, Food...|  Philadelphia|{7:0-21:0, 7:0-20...|      1|   39.9555052|   -75.1555641|  St Honore Pastries|      19107|          80|  4.0|   PA|\n",
      "|       101 Walnut St|{null, null, null...|mWMc6_wTdE0EUBKIG...|Brewpubs, Breweri...|    Green Lane|{12:0-22:0, null,...|      1|   40.3381827|   -75.4716585|Perkiomen Valley ...|      18054|          13|  4.5|   PA|\n",
      "|       615 S Main St|{null, null, u'no...|CF33F8-E6oudUQ46H...|Burgers, Fast Foo...|  Ashland City|{9:0-0:0, 0:0-0:0...|      1|    36.269593|    -87.058943|      Sonic Drive-In|      37015|           6|  2.0|   TN|\n",
      "|8522 Eager Road, ...|{null, null, null...|n_0UpQx1hsNbnPUSl...|Sporting Goods, F...|     Brentwood|{10:0-18:0, 0:0-0...|      1|    38.627695|    -90.340465|     Famous Footwear|      63144|          13|  2.5|   MO|\n",
      "|  400 Pasadena Ave S|                null|qkRM_2X51Yqxk3btl...|Synagogues, Relig...|St. Petersburg|{9:0-17:0, 9:0-17...|      1|     27.76659|    -82.732983|      Temple Beth-El|      33707|           5|  3.5|   FL|\n",
      "|   8025 Mackenzie Rd|{null, null, u'fu...|k0hlBqXX-Bt0vf1op...|Pubs, Restaurants...|        Affton|                null|      0|   38.5651648|   -90.3210868|Tsevi's Pub And G...|      63123|          19|  3.0|   MO|\n",
      "| 2312 Dickerson Pike|{null, null, u'no...|bBDDEgkFA1Otx9Lfe...|Ice Cream & Froze...|     Nashville|{6:0-16:0, 0:0-0:...|      1|   36.2081024|   -86.7681696|      Sonic Drive-In|      37207|          10|  1.5|   TN|\n",
      "|21705 Village Lak...|{null, null, null...|UJsufbvfyfONHeWdv...|Department Stores...| Land O' Lakes|{9:30-21:30, 9:30...|      1|28.1904587953|-82.4573802199|           Marshalls|      34639|           6|  3.5|   FL|\n",
      "|                    |{null, null, 'non...|eEOYSgkmpB90uNA7l...|Vietnamese, Food,...|     Tampa Bay|{11:0-14:0, 11:0-...|      1|   27.9552692|   -82.4563199|Vietnamese Food T...|      33602|          10|  4.0|   FL|\n",
      "|        8901 US 31 S|{null, null, 'non...|il_Ro8jwPlHresjw9...|American (Traditi...|  Indianapolis|{6:0-22:0, 6:0-22...|      1|39.6371332838| -86.127217412|             Denny's|      46227|          28|  2.5|   IN|\n",
      "|   15 N Missouri Ave|{null, null, null...|jaxMSoInw8Poo3XeM...|General Dentistry...|    Clearwater|{null, 7:30-15:30...|      1|    27.966235|    -82.787412|        Adams Dental|      33755|          10|  5.0|   FL|\n",
      "|       2575 E Bay Dr|{null, null, u'no...|0bPLkL0QhhPO5kt1_...|Food, Delis, Ital...|         Largo|{10:0-20:0, 10:0-...|      0|   27.9161159|   -82.7604608|Zio's Italian Market|      33771|         100|  4.5|   FL|\n",
      "|         205 Race St|{null, null, 'ful...|MUTTqe8uqyMdBl186...|Sushi Bars, Resta...|  Philadelphia|{13:30-23:0, null...|      1|    39.953949|   -75.1432262|            Tuna Bar|      19106|         245|  4.0|   PA|\n",
      "|     625 N Stone Ave|{null, null, null...|rBmpy_Y1UbBx8ggHl...|Automotive, Auto ...|        Tucson|{8:0-17:0, 0:0-0:...|      1|   32.2298719|  -110.9723419|Arizona Truck Out...|      85705|          10|  4.5|   AZ|\n",
      "|        712 Adams St|{null, null, null...|M0XSSHqrASOnhgbWD...|Vape Shops, Tobac...|   New Orleans|{10:0-19:0, 10:0-...|      1|29.9414679565| -90.129952757|      Herb Import Co|      70118|           5|  4.0|   LA|\n",
      "|     1241 Airline Dr|                null|8wGISYjYkE2tSqn3c...|Automotive, Car R...|        Kenner|{8:0-17:0, 8:0-17...|      1|    29.981183|   -90.2540123|    Nifty Car Rental|      70062|          14|  3.5|   LA|\n",
      "|       1224 South St|{null, null, u'no...|ROeacJQwBeh05Rqg7...| Korean, Restaurants|  Philadelphia|{11:30-20:30, 11:...|      1|    39.943223|    -75.162568|                 BAP|      19147|         205|  4.5|   PA|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL:\n",
    "# Reduce resource usage and make queries run faster\n",
    "# by only using a small sample of the dataframe\n",
    "# and overwriting previous variable \"df\".\n",
    "# Useful while developing, not so much to\n",
    "# provide final answers. Therefore: Remember to \n",
    "# to re-read the df when done developing code using\n",
    "# df = spark.read etc like above.\n",
    "reviews = reviews.sample(withReplacement=False, fraction=1/50)\n",
    "\n",
    "# Get number of rows after sampling:\n",
    "\n",
    "business.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20087cdb-d6f2-4aab-9fe2-bf3f072044db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:04:59.701181Z",
     "iopub.status.busy": "2023-12-22T21:04:59.700481Z",
     "iopub.status.idle": "2023-12-22T21:05:12.081327Z",
     "shell.execute_reply": "2023-12-22T21:05:12.079343Z",
     "shell.execute_reply.started": "2023-12-22T21:04:59.701130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------+\n",
      "|         business_id|number_of_influencer_reviews|\n",
      "+--------------------+----------------------------+\n",
      "|Ety2Z0CImO6FYDV6L...|                           1|\n",
      "|kPG6r0h73sPgXBei0...|                           1|\n",
      "|lXCFcmhoRsyW-mnzz...|                           1|\n",
      "|8vvMGaJ5biveUVE6-...|                           1|\n",
      "|fnO03-RX7UDC1TzXE...|                           1|\n",
      "|r3If7d4wAvYIbltQf...|                           1|\n",
      "|3u5ri1nLnDyFeGEHE...|                           1|\n",
      "|2KIDQyTh-HzLxOUED...|                           1|\n",
      "|ltBBYdNzkeKdCNPDA...|                           1|\n",
      "|T-uphPk44OFt6sR8F...|                           1|\n",
      "|z06yuwkwWjgXAxnlZ...|                           1|\n",
      "|xwmdDSPBvFAM9D74f...|                           1|\n",
      "|mzZ_WTb2zvyJMBkm8...|                           1|\n",
      "|qmzHJVxvr63lyybBJ...|                           1|\n",
      "|tJPaPZwRqFmDSEc8V...|                           2|\n",
      "|yy3XuGkFIowGr3jL4...|                           1|\n",
      "|pG5hLBK49Oz2TLX2q...|                           1|\n",
      "|pym7c6ZFEtmoH16xN...|                           1|\n",
      "|8-j10_I745ITrczbk...|                           1|\n",
      "|2wvN4OxbXvQIohAZV...|                           1|\n",
      "+--------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "number of businesses with more than 5 influencer reviews: 3\n",
      "+--------------------+----------------------------+\n",
      "|                name|number_of_influencer_reviews|\n",
      "+--------------------+----------------------------+\n",
      "|Louis Armstrong N...|                           7|\n",
      "|Reading Terminal ...|                           6|\n",
      "|       Pat O'Brien’s|                           6|\n",
      "+--------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.1.1:\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "temp = business.agg(sum(\"review_count\").alias(\"total amount of reviews\"))\n",
    "\n",
    "#temp.show()\n",
    "\n",
    "#reviewsCount.show()\n",
    "\n",
    "#3.1.2:\n",
    "\n",
    "\n",
    "fiveStarReviews = business.select(\"name\",\"stars\",\"review_count\").filter((business[\"stars\"] == 5.0) & (business[\"review_count\"] >= 500))\n",
    "\n",
    "#fiveStarReviews.show()\n",
    "\n",
    "#3.1.3:\n",
    "\n",
    "#Analyze user.json to find the influencers who have written more than 1000 reviews. \n",
    "#The output should be in the form of a Spark Table/DataFrame of user id.\n",
    "\n",
    "specialUsers = users.filter(users[\"review_count\"] > 1000).select(\"user_id\")\n",
    "\n",
    "#creating view containing the influencers\n",
    "specialUsers.createOrReplaceTempView(\"influencers\")\n",
    "\n",
    "#3.1.4:\n",
    "\n",
    "# Analyze review.json, business.json, and a view created from your answer to Q3 to find the businesses that have been reviewed by more than 5 influencer users.\n",
    "\n",
    "#joining reviews with influencers on id\n",
    "influencer_reviews = reviews.join(specialUsers, reviews[\"user_id\"] == specialUsers[\"user_id\"], \"inner\")\n",
    "\n",
    "#counting how many influencer reviews each business have:\n",
    "business_influencerRev_count = influencer_reviews.groupBy(\"business_id\").count().withColumnRenamed(\"count\", \"number_of_influencer_reviews\")\n",
    "\n",
    "business_influencerRev_count.show()\n",
    "\n",
    "#selecting only businesses that have more than five influencer reviews:\n",
    "\n",
    "more_than_five= business_influencerRev_count.filter(business_influencerRev_count[\"number_of_influencer_reviews\"] > 5)\n",
    "\n",
    "print(\"number of businesses with more than 5 influencer reviews: \" + str(more_than_five.count()))\n",
    "\n",
    "#showing which businesses have more than five:\n",
    "\n",
    "final_result = more_than_five.join(business, more_than_five[\"business_id\"] == business[\"business_id\"], \"inner\")\n",
    "\n",
    "final_result.select( \"name\", \"number_of_influencer_reviews\").show()\n",
    "\n",
    "\n",
    "#3.1.5:\n",
    "\n",
    "# Analyze review.json and user.json to find an ordered list of users based on the average star counts they have given in all their reviews.\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "first= reviews.join(users, reviews[\"user_id\"] == users[\"user_id\"]).select(\"name\", \"stars\")\n",
    "\n",
    "second = first.groupBy(\"name\").agg(avg(\"stars\"))\n",
    "second = second.withColumnRenamed(\"avg(stars)\", \"average\") \n",
    "\n",
    "final = second.orderBy(\"average\", ascending=False)\n",
    "\n",
    "#final.show()\n",
    "\n",
    "#3.2.1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d258fa6-1402-4c3a-a252-a9dbfd4a87d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07983acb-15d8-45ea-9d12-3bc8759280b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:05:12.084722Z",
     "iopub.status.busy": "2023-12-22T21:05:12.084224Z",
     "iopub.status.idle": "2023-12-22T21:05:13.691909Z",
     "shell.execute_reply": "2023-12-22T21:05:13.691113Z",
     "shell.execute_reply.started": "2023-12-22T21:05:12.084668Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of reviews containing a variant of authentic is: 1.66%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#3.2.1: \n",
    "\n",
    "# What is the percentage of reviews containing a variant of the word \"authentic\"? \n",
    "\n",
    "#first we define a new column with boolean values called \"is_aut\", that checks if a review contains either a string or a substring with \"authentic\"\n",
    "#this way variants of authentic like \"authenticity\" is checked:\n",
    "\n",
    "authentic_variants = [\"authentic\", \"authentik\", \"authentique\"]\n",
    "\n",
    "authentic = reviews.withColumn(\"is_aut\", col(\"text\").contains(\"authentic\"))\n",
    "\n",
    "#counting total amount of reviews:\n",
    "all_reviews = reviews.count()\n",
    "\n",
    "#finding amount of reviews containing a variant of authentic:\n",
    "authentic_reviews = authentic.filter(col(\"is_aut\") == True).count()\n",
    "\n",
    "#finding the percentage by dividing the number of authentic revs with all revs and multiply the result with 100\n",
    "percentage = (authentic_reviews / all_reviews) * 100\n",
    "\n",
    "print(\"percentage of reviews containing a variant of authentic is: \" + str(round(percentage, 2)) + \"%\")\n",
    "\n",
    "\n",
    "# How many reviews contain the string \"legitimate\" grouped by type of cuisine?\n",
    "\n",
    "legitimate = reviews.withColumn(\"is_legit\", col(\"text\").contains(\"legitimate\"))\n",
    "\n",
    "l_true = legitimate.filter(col(\"is_legit\") == True)\n",
    "\n",
    "l_true.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9f19d5-ba4c-4bf2-9144-ea6029e85191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:05:13.696328Z",
     "iopub.status.busy": "2023-12-22T21:05:13.695905Z",
     "iopub.status.idle": "2023-12-22T21:05:14.006493Z",
     "shell.execute_reply": "2023-12-22T21:05:14.005435Z",
     "shell.execute_reply.started": "2023-12-22T21:05:13.696302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139887"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authentic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fa7dcc-0146-4a77-89e5-d173c754d631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:05:14.009108Z",
     "iopub.status.busy": "2023-12-22T21:05:14.008258Z",
     "iopub.status.idle": "2023-12-22T21:05:14.975292Z",
     "shell.execute_reply": "2023-12-22T21:05:14.973725Z",
     "shell.execute_reply.started": "2023-12-22T21:05:14.009059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of legit businesses: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 200)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cuisines = [\"African\", \"American\", \"British\", \"Carribbean\", \"Italian\", \"Indian\", \"Korean\", \"Mexican\", \"Thai\", \"Vietnamese\", \"Indonesian\", \n",
    "                 \"French\", \"British\", \"Nordic\", \"Chinese\", \"Greek\", \"Irish\", \"Portuguese\", \"Spanish\", \"Turkish\", \"Asian\", \"Middle-Eastern\"]\n",
    "\n",
    "#First i create an empty table\n",
    "filterCon = col(\"categories\").contains(cuisines[0])\n",
    "\n",
    "\n",
    "# i use this loop with the I operator to be able to dynamically change the filter condition to each type of cuisine in the list \n",
    "for name in cuisines[1:]:\n",
    "    filterCon = filterCon | col(\"categories\").contains(name)\n",
    "\n",
    "# using the created condition to filter through businesses and find all the cuisines in the list\n",
    "f_business = business.filter(filterCon)\n",
    "\n",
    "# now i join the existing \n",
    "l_business = l_true.join(f_business, l_true[\"business_id\"] == f_business[\"business_id\"], \"inner\")\n",
    "\n",
    "#l_business.select(\"name\",\"categories\",\"is_legit\").show()\n",
    "\n",
    "#How many reviews contain the string \"legitimate\" grouped by type of cuisine?\n",
    "\n",
    "print(\"number of legit businesses: \" + str(l_business.count()))\n",
    "\n",
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da9e618-96be-45f6-9a99-bc6038511416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:05:14.978438Z",
     "iopub.status.busy": "2023-12-22T21:05:14.977531Z",
     "iopub.status.idle": "2023-12-22T21:05:23.507793Z",
     "shell.execute_reply": "2023-12-22T21:05:23.505758Z",
     "shell.execute_reply.started": "2023-12-22T21:05:14.978385Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37674\n",
      "+-----+---------+\n",
      "|state|aut_count|\n",
      "+-----+---------+\n",
      "|   PA|      157|\n",
      "|   NJ|      103|\n",
      "|   FL|       84|\n",
      "|   PA|       61|\n",
      "|   MO|       61|\n",
      "|   TN|       34|\n",
      "|   IL|       30|\n",
      "|   NJ|       26|\n",
      "|   IN|       25|\n",
      "|   LA|       24|\n",
      "|   AZ|       14|\n",
      "|   DE|       14|\n",
      "|   NV|       14|\n",
      "|   CA|       12|\n",
      "|   AB|        7|\n",
      "|   ID|        6|\n",
      "|   SD|        1|\n",
      "|   MA|        1|\n",
      "+-----+---------+\n",
      "\n",
      "+-----+------+-----------+--------------------+\n",
      "|state|is_aut|north_south|          percentage|\n",
      "+-----+------+-----------+--------------------+\n",
      "|   PA|  true|      North| 0.41673302542867763|\n",
      "|   NJ|  true|      South|   0.273398099485056|\n",
      "|   FL|  true|      South|  0.2229654403567447|\n",
      "|   PA|  true|      South| 0.16191537930668365|\n",
      "|   MO|  true|      South| 0.16191537930668365|\n",
      "|   TN|  true|      South| 0.09024791633487286|\n",
      "|   IL|  true|      South| 0.07963051441312312|\n",
      "|   NJ|  true|      North| 0.06901311249137336|\n",
      "|   IN|  true|      South| 0.06635876201093592|\n",
      "|   LA|  true|      South| 0.06370441153049848|\n",
      "|   AZ|  true|      South| 0.03716090672612412|\n",
      "|   DE|  true|      South| 0.03716090672612412|\n",
      "|   NV|  true|      South| 0.03716090672612412|\n",
      "|   CA|  true|      South| 0.03185220576524924|\n",
      "|   AB|  true|      North| 0.01858045336306206|\n",
      "|   ID|  true|      North| 0.01592610288262462|\n",
      "|   MA|  true|      South|0.002654350480437437|\n",
      "|   SD|  true|      South|0.002654350480437437|\n",
      "+-----+------+-----------+--------------------+\n",
      "\n",
      "+-----------+------------+------------------+\n",
      "|north_south|aut_business|        percentage|\n",
      "+-----------+------------+------------------+\n",
      "|      South|         478| 70.91988130563797|\n",
      "|      North|         196|29.080118694362017|\n",
      "+-----------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Is there a difference in the amount of authenticity language used in the different areas? (e.g., by state, north/south, urban/rural)\n",
    "\n",
    "# Defining a set of authentic keywords\n",
    "\n",
    "from pyspark.sql.functions import count, col, when\n",
    "\n",
    "aut_keywords = [\"authentic\", \"genuine\", \"real\", \"traditional\", \"original\", \"actual\", \"true\", \"legitimate\", \"valid\", \"reliable\", \"accurate\", \"honest\"]\n",
    "\n",
    "#using r.like method that takes a regex expression\n",
    "#the vertical bar | will act as an or operator, that will match any of the authentic keywords\n",
    "aut = reviews.withColumn(\"is_aut\", col(\"text\").rlike(\"|\".join(aut_keywords)))\n",
    "\n",
    "true_count = aut.filter(col(\"is_aut\") == True).count()\n",
    "\n",
    "print(true_count)\n",
    "\n",
    "# https://simple.wikipedia.org/wiki/Northern_United_States\n",
    "\n",
    "# https://www.google.com/maps/place/USA/@35.6618607,-116.9516466,4z/data=!3m1!4b1!4m6!3m5!1s0x54eab584e432360b:0x1c3bb99243deb742!8m2!3d37.09024!4d-95.712891!16zL20vMDljN3cw?entry=ttu\n",
    "\n",
    "#Very hard to divide business by north or south with information on google. Therefore i took a threshold of 40 on latitude\n",
    "newbusiness = business.withColumn(\n",
    "    \"north_south\",\n",
    "    when(col(\"latitude\") > 40, \"North\")\n",
    "    .when(col(\"latitude\") < 40, \"South\")\n",
    ")\n",
    "\n",
    "#using the categorized businesses in north and south\n",
    "aut_business = newbusiness.join(aut, on=\"business_id\")\n",
    "\n",
    "#now i cube  state, city, authenticity and if they are south or north\n",
    "aut_cube = aut_business.cube(\"state\", \"city\", \"is_aut\", \"north_south\")\n",
    "\n",
    "counts = aut_cube.agg(count(\"*\").alias(\"review_count\"))\n",
    "\n",
    "filter_counts = counts.na.drop()\n",
    "\n",
    "\n",
    "#difference in authentic businesses by each state\n",
    "\n",
    "temp_aut_b_count = filter_counts.filter(col(\"is_aut\") == True)\n",
    "\n",
    "# now i group and order by descending states in terms of count\n",
    "state_aut = temp_aut_b_count.groupBy(\"state\", \"is_aut\", \"north_south\").agg(count(\"*\").alias(\"aut_count\"))\n",
    "\n",
    "state_count = state_aut.orderBy(\"aut_count\", ascending=False)\n",
    "\n",
    "# Show which state has the highest amount of authentic businesses\n",
    "state_count.select(\"state\", \"aut_count\").show()\n",
    "\n",
    "#showing a percentage column for each state\n",
    "percentage = temp_aut_b_count.groupBy(\"state\", \"is_aut\", \"north_south\").agg(count(\"*\").alias(\"percentage\"))\n",
    "\n",
    "aut_state = percentage.withColumn(\n",
    "    \"percentage\",\n",
    "    (col(\"percentage\") / true_count * 100)\n",
    ")\n",
    "\n",
    "aut_state = aut_state.orderBy(\"percentage\", ascending=False)\n",
    "\n",
    "aut_state.show()\n",
    "\n",
    "#now i want to show two rows containing the whole percentage of north and south businesses, that are authentic\n",
    "\n",
    "north_south = temp_aut_b_count.groupBy(\"north_south\").agg(\n",
    "    F.count(\"*\").alias(\"aut_business\")\n",
    ")\n",
    "\n",
    "#using the total amount of businesses for division\n",
    "total = temp_aut_b_count.count()\n",
    "\n",
    "#calculating percentage for north and south\n",
    "final = north_south.withColumn(\n",
    "    \"percentage\", (col(\"aut_business\") / total* 100)\n",
    ")\n",
    "\n",
    "final.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5288823-f4aa-4b25-ae3d-ddd6a6d502a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affa9d6e-adff-4e48-8ffc-9d55e09ccc5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:05:23.510604Z",
     "iopub.status.busy": "2023-12-22T21:05:23.510076Z",
     "iopub.status.idle": "2023-12-22T21:05:31.801042Z",
     "shell.execute_reply": "2023-12-22T21:05:31.799322Z",
     "shell.execute_reply.started": "2023-12-22T21:05:23.510567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+-------------+------------------+------------------+\n",
      "|       cuisine|has_negative|is_authentic|total_reviews|    aut_percentage|    neg_percentage|\n",
      "+--------------+------------+------------+-------------+------------------+------------------+\n",
      "|      European|        2018|         417|         6676|6.2462552426602755|30.227681246255244|\n",
      "|South American|        1496|         698|         5286|13.204691638289823|28.301172909572454|\n",
      "|         Asian|        2083|         749|         7632| 9.813941299790356|27.292976939203356|\n",
      "+--------------+------------+------------+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can you identify a difference in the relationship between authenticity language and typically negative words ?\n",
    "# in restaurants serving _south american or asian cuisine_ compared to restaurants serving _european cuisine_? And to what degree?\n",
    "\n",
    "#Find all the south american and asian cuisine containing authentic and negative words\n",
    "#Find all the european cuising authentic and negative words\n",
    "\n",
    "bad_words = [\"bad\",\"awful\", \"nasty\", \"terrible\", \"dirty\", \"kitsch\", \"cheap\", \"rude\", \"simple\"]\n",
    "    \n",
    "#copied down for overview\n",
    "authenticity_keywords = [\"authentic\", \"legitimate\"]\n",
    "authenticity = reviews.withColumn(\"is_aut\", col(\"text\").rlike(\"|\".join(authenticity_keywords)))\n",
    "\n",
    "#combining with negative words\n",
    "\n",
    "neg_words = authenticity.withColumn(\"has_neg\", col(\"text\").rlike(\"|\".join(bad_words)))\n",
    "\n",
    "#now i filter on each type of cuisine to find business related to their continent\n",
    "\n",
    "european = [\n",
    "    'Italian', 'French', 'Spanish', 'German', 'Greek', 'Portuguese', 'British', 'Russian', 'Hungarian', \n",
    "    'Polish', 'Dutch', 'Swedish', 'Belgian', 'Turkish', 'Irish', 'Romanian', 'Austrian', 'Swiss', \n",
    "    'Norwegian', 'Czech', 'Finnish', 'Danish', 'Ukrainian', 'Serbian', 'Slovakian', 'Croatian', 'Bulgarian',\n",
    "    'Lithuanian', 'Latvian', 'Estonian', 'Slovenian', 'Macedonian', 'Maltese', 'Moldovan', 'Luxembourger',\n",
    "    'Icelandic', 'Albanian', 'Andorran', 'Bosnian', 'Georgian'\n",
    "]\n",
    "\n",
    "south_american= [\n",
    "    'Brazilian', 'Argentine', 'Peruvian', 'Colombian', 'Venezuelan', 'Chilean', 'Ecuadorian', 'Bolivian', \n",
    "    'Paraguayan', 'Uruguayan', 'Guyanese', 'Surinamese', 'Columbian', 'French Guianese', 'Falkland Islander',\n",
    "    'Belizean', 'Nicaraguan', 'Honduran', 'Costa Rican', 'Panamanian', 'Guatemalan', 'Salvadoran', 'Mexican',\n",
    "    'Cuban', 'Jamaican', 'Haitian', 'Bahamian', 'Trinidadian', 'Tobagonian', 'Barbadian', 'Antiguan',\n",
    "    'Barbudan', 'Dominican', 'Grenadian', 'Vincentian', 'Saint Lucian', 'Kittitian', 'Nevisian', 'Guyanese',\n",
    "    'Surinamese'\n",
    "]\n",
    "\n",
    "asian= [\n",
    "    'Chinese', 'Indian', 'Japanese', 'Thai', 'Indonesian', 'Filipino', 'Vietnamese', 'Korean', 'Malaysian', \n",
    "    'Sri Lankan', 'Singaporean', 'Bangladeshi', 'Pakistani', 'Nepalese', 'Cambodian', 'Myanmar', 'Afghan', \n",
    "    'Laotian', 'Taiwanese', 'Mongolian', 'Bhutanese', 'Bruneian', 'Maldivian', 'Tibetan', 'Burmese', \n",
    "    'Kazakhstani', 'Kyrgyz', 'Tajik', 'Turkmen', 'Uzbekistani', 'Kuwaiti', 'Omani', 'Qatari', 'Saudi', \n",
    "    'Emirati', 'Yemeni', 'Iranian', 'Iraqi', 'Israeli'\n",
    "]\n",
    "\n",
    "businesss = business.filter(col(\"categories\").contains(\"Restaurants\"))\n",
    "\n",
    "euro_business = business.filter(col(\"categories\").rlike(\"|\".join(european)))\n",
    "sa_business = business.filter(col(\"categories\").rlike(\"|\".join(south_american)))\n",
    "asian_business = business.filter(col(\"categories\").rlike(\"|\".join(asian)))\n",
    "\n",
    "# joining the reviews with aut and bad words\n",
    "euro = neg_words.join(euro_business, on=\"business_id\")\n",
    "sa = neg_words.join(sa_business, on=\"business_id\")\n",
    "asian = neg_words.join(asian_business, on=\"business_id\")\n",
    "\n",
    "#now i need to count how many authentic and neg reviews each cuisine has\n",
    "\n",
    "#starting with euro\n",
    "\n",
    "euro_dict = {\n",
    "    \"cuisine\": \"European\",\n",
    "    \"is_authentic\": euro.filter(col(\"is_aut\") == True).count(),\n",
    "    \"has_negative\": euro.filter(col(\"has_neg\") == True).count(),\n",
    "    \"total_reviews\": euro_business.count()\n",
    "}\n",
    "\n",
    "sa_dict = {\n",
    "    \"cuisine\": \"South American\",\n",
    "    \"is_authentic\": sa.filter(col(\"is_aut\") == True).count(),\n",
    "    \"has_negative\": sa.filter(col(\"has_neg\") == True).count(),\n",
    "    \"total_reviews\": sa_business.count()\n",
    "}\n",
    "\n",
    "asia_dict = {\n",
    "    \"cuisine\": \"Asian\",\n",
    "    \"is_authentic\": asian.filter(col(\"is_aut\") == True).count(),\n",
    "    \"has_negative\": asian.filter(col(\"has_neg\") == True).count(),\n",
    "    \"total_reviews\": asian_business.count()\n",
    "}\n",
    "\n",
    "\n",
    "all_cuisines = [euro_dict, sa_dict, asia_dict]\n",
    "\n",
    "#creating new dataframe containing each of the cuisines \n",
    "\n",
    "final = spark.createDataFrame(all_cuisines)\n",
    "final = final.withColumn(\"aut_percentage\", (col(\"is_authentic\") / col(\"total_reviews\") * 100))\n",
    "final = final.withColumn(\"neg_percentage\", (col(\"has_negative\") / col(\"total_reviews\") * 100))\n",
    "\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20a26121-222d-4930-81c8-3c46f55bbd40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T21:32:30.184285Z",
     "iopub.status.busy": "2023-12-22T21:32:30.183513Z",
     "iopub.status.idle": "2023-12-22T21:32:30.257851Z",
     "shell.execute_reply": "2023-12-22T21:32:30.257055Z",
     "shell.execute_reply.started": "2023-12-22T21:32:30.184233Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "#selecting which columns to preprocess: \n",
    "\n",
    "new_business = business.drop(\"stars\")\n",
    "\n",
    "preprocess = new_business.join(neg_words, on=\"business_id\", how=\"inner\")\n",
    "\n",
    "#first i want to see how much text is a feature importance for rating\n",
    "preprocess1 = preprocess.select(\"text\", \"stars\")\n",
    "#then i want to see how much other features combined \n",
    "preprocess2 = preprocess.select(\"text\", \"city\", \"state\", \"cool\", \"funny\", \"useful\", \"is_aut\", \"has_neg\", \"stars\", \"is_open\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f8b10d9-3dce-41db-96f7-77d184767c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T23:38:36.251797Z",
     "iopub.status.busy": "2023-12-22T23:38:36.251028Z",
     "iopub.status.idle": "2023-12-22T23:38:37.540553Z",
     "shell.execute_reply": "2023-12-22T23:38:37.539287Z",
     "shell.execute_reply.started": "2023-12-22T23:38:36.251744Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|stars|         listOfWords|ListWithoutStopWords|           TFvectors|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|If you decide to ...|  3.0|[if, you, decide,...|[decide, eat, her...|(10000,[92,129,27...|(10000,[92,129,27...|\n",
      "|Say you decide th...|  5.0|[say, you, decide...|[say, decide, nee...|(10000,[453,551,5...|(10000,[453,551,5...|\n",
      "|Took a while to g...|  4.0|[took, a, while, ...|[took, get, actua...|(10000,[1369,1388...|(10000,[1369,1388...|\n",
      "|Fantastic sliders...|  5.0|[fantastic, slide...|[fantastic, slide...|(10000,[1285,1369...|(10000,[1285,1369...|\n",
      "|I was here a few ...|  3.0|[i, was, here, a,...|[times, review., ...|(10000,[750,1221,...|(10000,[750,1221,...|\n",
      "|Awesome detailing...|  5.0|[awesome, detaili...|[awesome, detaili...|(10000,[871,2007,...|(10000,[871,2007,...|\n",
      "|In short, the foo...|  3.0|[in, short,, the,...|[short,, food, de...|(10000,[42,313,47...|(10000,[42,313,47...|\n",
      "|Whole family got ...|  1.0|[whole, family, g...|[whole, family, g...|(10000,[627,903,3...|(10000,[627,903,3...|\n",
      "|This place sucks!...|  1.0|[this, place, suc...|[place, sucks!, o...|(10000,[94,198,52...|(10000,[94,198,52...|\n",
      "|We loved the food...|  4.0|[we, loved, the, ...|[loved, food,, se...|(10000,[505,569,6...|(10000,[505,569,6...|\n",
      "|I'm not sure what...|  1.0|[i'm, not, sure, ...|[sure, changed, l...|(10000,[1127,1376...|(10000,[1127,1376...|\n",
      "|We went on a Frid...|  3.0|[we, went, on, a,...|[went, friday, ni...|(10000,[524,533,1...|(10000,[524,533,1...|\n",
      "|Went here for din...|  5.0|[went, here, for,...|[went, dinner, op...|(10000,[18,54,269...|(10000,[18,54,269...|\n",
      "|I can't recall th...|  5.0|[i, can't, recall...|[recall, day, wen...|(10000,[199,273,3...|(10000,[199,273,3...|\n",
      "|I am so glad we h...|  5.0|[i, am, so, glad,...|[glad, wedding, h...|(10000,[12,15,91,...|(10000,[12,15,91,...|\n",
      "|On my first day i...|  4.0|[on, my, first, d...|[first, day, new,...|(10000,[199,222,2...|(10000,[199,222,2...|\n",
      "|First impression:...|  4.0|[first, impressio...|[first, impressio...|(10000,[15,59,115...|(10000,[15,59,115...|\n",
      "|I eat at Pina's o...|  4.0|[i, eat, at, pina...|[eat, pina's, tak...|(10000,[642,877,1...|(10000,[642,877,1...|\n",
      "|This was a nice s...|  4.0|[this, was, a, ni...|[nice, spot, side...|(10000,[646,1004,...|(10000,[646,1004,...|\n",
      "|Still running on ...|  3.0|[still, running, ...|[still, running, ...|(10000,[80,374,38...|(10000,[80,374,38...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#starting to preprocess my text column as a \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#preprocessing the text column into a list or words\n",
    "#removing stopwords like \"is, are\"... \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"listOfWords\")\n",
    "stopWords = StopWordsRemover(inputCol=\"listOfWords\", outputCol=\"ListWithoutStopWords\")\n",
    "\n",
    "#converting the list of words into term frequency vectors, that represents each words frequency in the each review\n",
    "hashingTF = HashingTF(inputCol=\"ListWithoutStopWords\", outputCol=\"TFvectors\", numFeatures=10000)\n",
    "#using idf to rescale the TF vector on that particular review. It uses TF vectors from all the reviews to compute the idf values for each TF \n",
    "#it calculates how frequent each term occurs across all reviews \n",
    "idf = IDF(inputCol=\"TFvectors\", outputCol=\"features\")\n",
    "\n",
    "#using a pipeline to transform / preprocess the data\n",
    "pipeline = Pipeline(stages=[tokenizer, stopWords, hashingTF, idf])\n",
    "pipeline_model = pipeline.fit(preprocess1)\n",
    "rating = pipeline_model.transform(preprocess1)\n",
    "\n",
    "rating.show()\n",
    "\n",
    "#appending each stage to a list to print later: \n",
    "\n",
    "stages = [] \n",
    "\n",
    "for step in pipeline_model.stages:\n",
    "    name = step.__class__.__name__ \n",
    "    stages.append(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e52331a-64bf-4c1f-b4b6-1834d59b5b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T23:59:30.131055Z",
     "iopub.status.busy": "2023-12-22T23:59:30.130265Z",
     "iopub.status.idle": "2023-12-22T23:59:39.948950Z",
     "shell.execute_reply": "2023-12-22T23:59:39.947293Z",
     "shell.execute_reply.started": "2023-12-22T23:59:30.131003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing stages in the pipeline: ['Tokenizer', 'StopWordsRemover', 'HashingTF', 'IDFModel']\n",
      "Using Model: LinearRegression\n",
      "Max Iterations:  10\n",
      "Input Features: ['text', 'stars']\n",
      "Mean Squared Error: 1.1116060331452502\n",
      "Root Mean Squared Error: 1.0543272893865785\n",
      "R2-score:  0.4867709006636458\n",
      "Mean Absolute Error: 0.8324936276277384\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rating = rating.select(\"features\", \"stars\")\n",
    "\n",
    "train, test = rating.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LinearRegression(labelCol=\"stars\", maxIter=10)\n",
    "\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "#use our model to make predictions\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "print(\"Preprocessing stages in the pipeline: \" + str(stages))\n",
    "print(\"Using Model: LinearRegression\")\n",
    "print(\"Max Iterations: \", lr_model.getMaxIter())\n",
    "print(\"Input Features: \" + str(preprocess1.columns))\n",
    "#evaluating using metrics such as MSE, RSME, R2 and MAE\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"mse\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"rmse\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"r2\", predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R2-score: \", r2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"mae\", predictionCol=\"prediction\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1221ec66-312f-4d87-a505-cfb7eee7e898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T00:02:58.524615Z",
     "iopub.status.busy": "2023-12-23T00:02:58.523827Z",
     "iopub.status.idle": "2023-12-23T00:03:00.790781Z",
     "shell.execute_reply": "2023-12-23T00:03:00.789101Z",
     "shell.execute_reply.started": "2023-12-23T00:02:58.524548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         AllFeatures|stars|\n",
      "+--------------------+-----+\n",
      "|(10006,[92,129,27...|  3.0|\n",
      "|(10006,[453,551,5...|  5.0|\n",
      "|(10006,[1369,1388...|  4.0|\n",
      "|(10006,[1285,1369...|  5.0|\n",
      "|(10006,[750,1221,...|  3.0|\n",
      "|(10006,[871,2007,...|  5.0|\n",
      "|(10006,[42,313,47...|  3.0|\n",
      "|(10006,[627,903,3...|  1.0|\n",
      "|(10006,[94,198,52...|  1.0|\n",
      "|(10006,[505,569,6...|  4.0|\n",
      "|(10006,[1127,1376...|  1.0|\n",
      "|(10006,[524,533,1...|  3.0|\n",
      "|(10006,[18,54,269...|  5.0|\n",
      "|(10006,[199,273,3...|  5.0|\n",
      "|(10006,[12,15,91,...|  5.0|\n",
      "|(10006,[199,222,2...|  4.0|\n",
      "|(10006,[15,59,115...|  4.0|\n",
      "|(10006,[642,877,1...|  4.0|\n",
      "|(10006,[646,1004,...|  4.0|\n",
      "|(10006,[80,374,38...|  3.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "#using a pipeline to transform / preprocess the data\n",
    "pipeline2 = Pipeline(stages=[tokenizer, stopWords, hashingTF, idf])\n",
    "pipeline2_model = pipeline.fit(preprocess2)\n",
    "rating2 = pipeline2_model.transform(preprocess2)\n",
    "\n",
    "#appending each stage to a list to print later: \n",
    "stages = [] \n",
    "for step in pipeline_model.stages:\n",
    "    name = step.__class__.__name__ \n",
    "    stages.append(name) \n",
    "\n",
    "#converting state and city into numerical values and categorical data using one hot encoding\n",
    "stringIndex1 = StringIndexer(inputCol=\"city\", outputCol=\"city_index\", handleInvalid=\"keep\")\n",
    "indexed = stringIndex1.fit(rating2).transform(rating2)\n",
    "stringIndex2 = StringIndexer(inputCol=\"state\", outputCol=\"state_index\", handleInvalid=\"keep\")\n",
    "indexed = stringIndex2.fit(indexed).transform(indexed)\n",
    "encoder1 = OneHotEncoder(inputCol=\"city_index\", outputCol=\"city_encoded\")\n",
    "indexed_and_encoded = encoder1.fit(indexed).transform(indexed)\n",
    "encoder2 = OneHotEncoder(inputCol=\"state_index\", outputCol=\"state_encoded\")\n",
    "indexed_and_encoded = encoder2.fit(indexed_and_encoded).transform(indexed_and_encoded)\n",
    "\n",
    "#using the rest of the features aswell\n",
    "#input_features = [\"features\",\"cool\", \"funny\", \"useful\", \"is_aut\", \"has_neg\", \"city_encoded\", \"state_encoded\"]\n",
    "input_features1 = [\"features\",\"cool\", \"funny\", \"useful\", \"is_aut\", \"has_neg\", \"is_open\"]\n",
    "assembler = VectorAssembler(inputCols=input_features1, outputCol=\"AllFeatures\")\n",
    "final = assembler.transform(indexed_and_encoded)\n",
    "\n",
    "final = final.select(\"AllFeatures\", \"stars\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8192793d-dd25-4f3b-b9b6-0658d103c607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T00:03:04.746451Z",
     "iopub.status.busy": "2023-12-23T00:03:04.745681Z",
     "iopub.status.idle": "2023-12-23T00:03:17.069164Z",
     "shell.execute_reply": "2023-12-23T00:03:17.065861Z",
     "shell.execute_reply.started": "2023-12-23T00:03:04.746400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing stages in the pipeline: ['Tokenizer', 'StopWordsRemover', 'HashingTF', 'IDFModel']\n",
      "Using Model: LinearRegression\n",
      "Max Iterations:  10\n",
      "Vector Assember with following features ['features', 'cool', 'funny', 'useful', 'is_aut', 'has_neg', 'is_open']\n",
      "    *NOTE: 'features' contains the idf vectors of 'text' column\n",
      "Mean Squared Error: 1.090713962736012\n",
      "Root Mean Squared Error: 1.0443725210555916\n",
      "R2-score:  0.49641678073238416\n",
      "Mean Absolute Error: 0.8240674081593888\n"
     ]
    }
   ],
   "source": [
    "train, test = final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol='AllFeatures',labelCol=\"stars\", maxIter=10)\n",
    "\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "#use our model to make predictions\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "print(\"Preprocessing stages in the pipeline: \" + str(stages))\n",
    "print(\"Using Model: LinearRegression\")\n",
    "print(\"Max Iterations: \", lr_model.getMaxIter())\n",
    "print(\"Vector Assember with following features \" + str(input_features1))\n",
    "print(\"    *NOTE: 'features' contains the idf vectors of 'text' column\")\n",
    "#evaluating using metrics such as MSE, RSME, R2 and MAE to see how our model performs\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"mse\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"rmse\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"r2\", predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R2-score: \", r2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", metricName=\"mae\", predictionCol=\"prediction\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9abde57-f18a-45dd-85bc-7acb5032f81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T00:01:10.954458Z",
     "iopub.status.busy": "2023-12-23T00:01:10.953692Z",
     "iopub.status.idle": "2023-12-23T00:01:22.700543Z",
     "shell.execute_reply": "2023-12-23T00:01:22.698853Z",
     "shell.execute_reply.started": "2023-12-23T00:01:10.954408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing stages in the pipeline: ['Tokenizer', 'StopWordsRemover', 'HashingTF', 'IDFModel']\n",
      "Using Model: LogisticRegression\n",
      "Max Iterations:  10\n",
      "Standardization: True\n",
      "Vector Assember with following features ['features', 'cool', 'funny', 'useful', 'is_aut', 'has_neg', 'is_open', 'city_encoded', 'state_encoded']\n",
      "    *NOTE: 'features' contains the idf vectors of 'text' column\n",
      "Accuracy:  0.6311200057179616\n",
      "Precision: 0.6127413049165751\n",
      "F1-score:  0.6199410689182556\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "input_features2 = [\"features\",\"cool\", \"funny\", \"useful\", \"is_aut\", \"has_neg\", \"is_open\", \"city_encoded\", \"state_encoded\"]\n",
    "assembler = VectorAssembler(inputCols=input_features2, outputCol=\"AllFeatures\")\n",
    "final = assembler.transform(indexed_and_encoded)\n",
    "\n",
    "final = final.select(\"AllFeatures\", \"stars\")\n",
    "\n",
    "train, test = final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lg = LogisticRegression(featuresCol='AllFeatures', labelCol='stars', maxIter=10, standardization=True)\n",
    "\n",
    "lg_model = lg.fit(train)\n",
    "\n",
    "predictions = lg_model.transform(test)\n",
    "\n",
    "eval = MulticlassClassificationEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = eval.evaluate(predictions)\n",
    "eval = MulticlassClassificationEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = eval.evaluate(predictions)\n",
    "eval = MulticlassClassificationEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = eval.evaluate(predictions)\n",
    "\n",
    "print(\"Preprocessing stages in the pipeline: \" + str(stages))\n",
    "print(\"Using Model: LogisticRegression\")\n",
    "print(\"Max Iterations: \", lg_model.getMaxIter())\n",
    "print(\"Standardization: True\")\n",
    "print(\"Vector Assember with following features \" + str(input_features2))\n",
    "print(\"    *NOTE: 'features' contains the idf vectors of 'text' column\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393a226-772d-4477-aa88-2995dab41824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
